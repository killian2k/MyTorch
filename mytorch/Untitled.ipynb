{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14747204 0.66815595 0.70853712]\n",
      " [0.65021236 0.20659561 0.18273222]]\n",
      "[0.79768439 0.87475155 0.89126934]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "in_feature = 2\n",
    "out_feature = 3\n",
    "\n",
    "x = np.random.random((batch_size,in_feature))\n",
    "W = np.random.random((in_feature, out_feature))\n",
    "b = np.random.random((1,out_feature))\n",
    "print(W)\n",
    "print(W.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-36c6c9f901dd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-36c6c9f901dd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    a.sum()?\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a.sum()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      "  18. 19.]\n",
      " [20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n",
      "  38. 39.]\n",
      " [40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57.\n",
      "  58. 59.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.5, 29.5, 49.5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "a.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-58a91c2b6b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "a = [1,2] + 4\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "a.reshape((1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "eps = 1e-8\n",
    "x = None\n",
    "norm = None\n",
    "out = None\n",
    "in_feature = 15\n",
    "\n",
    "# The following attributes will be tested\n",
    "var = np.ones((1, in_feature))\n",
    "mean = np.zeros((1, in_feature))\n",
    "\n",
    "gamma = np.ones((1, in_feature))\n",
    "dgamma = np.zeros((1, in_feature))\n",
    "\n",
    "beta = np.zeros((1, in_feature))\n",
    "dbeta = np.zeros((1, in_feature))\n",
    "\n",
    "# inference parameters\n",
    "running_mean = np.zeros((1, in_feature))\n",
    "running_var = np.ones((1, in_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.arange(150.0).reshape((10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (10, 15) [[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.]\n",
      " [ 15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n",
      "   29.]\n",
      " [ 30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.  43.\n",
      "   44.]\n",
      " [ 45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.\n",
      "   59.]\n",
      " [ 60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.\n",
      "   74.]\n",
      " [ 75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.\n",
      "   89.]\n",
      " [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103.\n",
      "  104.]\n",
      " [105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118.\n",
      "  119.]\n",
      " [120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      "  134.]\n",
      " [135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148.\n",
      "  149.]]\n",
      "(1, 15)\n",
      "[67.5 68.5 69.5 70.5 71.5 72.5 73.5 74.5 75.5 76.5 77.5 78.5 79.5 80.5\n",
      " 81.5]\n",
      "var [1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25\n",
      " 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25]\n",
      "[[-1.         -0.98540146 -0.97122302 -0.95744681 -0.94405594 -0.93103448\n",
      "  -0.91836735 -0.90604027 -0.89403974 -0.88235294 -0.87096774 -0.85987261\n",
      "  -0.8490566  -0.83850932 -0.82822086]\n",
      " [-0.77777778 -0.76642336 -0.75539568 -0.74468085 -0.73426573 -0.72413793\n",
      "  -0.71428571 -0.70469799 -0.69536424 -0.68627451 -0.67741935 -0.66878981\n",
      "  -0.66037736 -0.65217391 -0.64417178]\n",
      " [-0.55555556 -0.54744526 -0.53956835 -0.53191489 -0.52447552 -0.51724138\n",
      "  -0.51020408 -0.5033557  -0.49668874 -0.49019608 -0.48387097 -0.47770701\n",
      "  -0.47169811 -0.46583851 -0.4601227 ]\n",
      " [-0.33333333 -0.32846715 -0.32374101 -0.31914894 -0.31468531 -0.31034483\n",
      "  -0.30612245 -0.30201342 -0.29801325 -0.29411765 -0.29032258 -0.2866242\n",
      "  -0.28301887 -0.27950311 -0.27607362]\n",
      " [-0.11111111 -0.10948905 -0.10791367 -0.10638298 -0.1048951  -0.10344828\n",
      "  -0.10204082 -0.10067114 -0.09933775 -0.09803922 -0.09677419 -0.0955414\n",
      "  -0.09433962 -0.0931677  -0.09202454]\n",
      " [ 0.11111111  0.10948905  0.10791367  0.10638298  0.1048951   0.10344828\n",
      "   0.10204082  0.10067114  0.09933775  0.09803922  0.09677419  0.0955414\n",
      "   0.09433962  0.0931677   0.09202454]\n",
      " [ 0.33333333  0.32846715  0.32374101  0.31914894  0.31468531  0.31034483\n",
      "   0.30612245  0.30201342  0.29801325  0.29411765  0.29032258  0.2866242\n",
      "   0.28301887  0.27950311  0.27607362]\n",
      " [ 0.55555556  0.54744526  0.53956835  0.53191489  0.52447552  0.51724138\n",
      "   0.51020408  0.5033557   0.49668874  0.49019608  0.48387097  0.47770701\n",
      "   0.47169811  0.46583851  0.4601227 ]\n",
      " [ 0.77777778  0.76642336  0.75539568  0.74468085  0.73426573  0.72413793\n",
      "   0.71428571  0.70469799  0.69536424  0.68627451  0.67741935  0.66878981\n",
      "   0.66037736  0.65217391  0.64417178]\n",
      " [ 1.          0.98540146  0.97122302  0.95744681  0.94405594  0.93103448\n",
      "   0.91836735  0.90604027  0.89403974  0.88235294  0.87096774  0.85987261\n",
      "   0.8490566   0.83850932  0.82822086]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape\", x.shape,x)\n",
    "mean = x.mean(axis=0)\n",
    "print(running_mean.shape)\n",
    "print(mean)\n",
    "var = np.var(x,axis=0)\n",
    "print(\"var\",var)\n",
    "#print(np.subtract(x,mean).shape)\n",
    "norm = ((x - mean)/(np.sqrt(np.power(mean,2) + eps)))\n",
    "out = gamma * norm + beta\n",
    "# Update running batch statistics\n",
    "running_mean = alpha * running_mean+ (1-alpha) * mean\n",
    "running_var = alpha * var + (1-alpha) * var\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b07936417e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mytorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbatchnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append('mytorch')\n",
    "from loss import *\n",
    "from activation import *\n",
    "from batchnorm import *\n",
    "from linear import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_feature, out_feature, weight_init_fn, bias_init_fn):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            W (np.array): (in feature, out feature)\n",
    "            dW (np.array): (in feature, out feature)\n",
    "            momentum_W (np.array): (in feature, out feature)\n",
    "\n",
    "            b (np.array): (1, out feature)\n",
    "            db (np.array): (1, out feature)\n",
    "            momentum_B (np.array): (1, out feature)\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = weight_init_fn(in_feature, out_feature)\n",
    "        self.b = bias_init_fn(out_feature)\n",
    "\n",
    "        # TODO: Complete these but do not change the names.\n",
    "        self.dW = np.zeros(None)\n",
    "        self.db = np.zeros(None)\n",
    "        \n",
    "        self.in_feature = 0\n",
    "        self.x = np.zeros(None)\n",
    "\n",
    "        self.momentum_W = np.zeros(None)\n",
    "        self.momentum_b = np.zeros(None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, in feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, out feature)\n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        self.in_feature = x.shape[0]\n",
    "        self.x = x\n",
    "        return x.dot(self.W) + self.b\n",
    "        \n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            delta (np.array): (batch size, out feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, in feature)\n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        self.dW = 1/ self.in_feature * self.x.T.dot(delta)\n",
    "        self.db = (1/self.in_feature * delta.sum(axis=0, keepdims=True))\n",
    "        out = delta.dot(self.W.T)\n",
    "        return out\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import math\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "\n",
    "    In all implementations, the state attribute must contain the result,\n",
    "    i.e. the output of forward (it will be tested).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an\n",
    "    # abstract base class for the others\n",
    "\n",
    "    # Note that these activation functions are scalar operations. I.e, they\n",
    "    # shouldn't change the shape of the input.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed\n",
    "    # to stay the same for AutoLab.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Might we need to store something before returning?\n",
    "        self.state = 1./(1+np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        # Maybe something we need later in here...\n",
    "        return self.state * (1-self.state)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1 - np.power(self.state,2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.where(x>0,x,0)\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return np.where(self.state>0,1.,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('mytorch')\n",
    "\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn,\n",
    "                 bias_init_fn, criterion, lr, momentum=0.0, num_bn_layers=0):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.num_bn_layers = num_bn_layers\n",
    "        self.bn = num_bn_layers > 0\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        # Initialize and add all your linear layers into the list 'self.linear_layers'\n",
    "        # (HINT: self.foo = [ bar(???) for ?? in ? ])\n",
    "        # (HINT: Can you use zip here?)\n",
    "        #Linear: (in_feature, out_feature, weight_init_fn, bias_init_fn)\n",
    "        activations\n",
    "        self.linear_layers = [Linear((t[0],t[1],weight_init_fn,bias_init_fn)) for t in zip(np.hstack((self.input_size,hiddens)), np.hstack((hiddens,self.output_size)))]\n",
    "\n",
    "        # If batch norm, add batch norm layers into the list 'self.bn_layers'\n",
    "        if self.bn:\n",
    "            BatchNorm(_in_feature)\n",
    "            self.bn_layers = self.num_bn_layers#NOT SURE\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, input_size)\n",
    "        Return:\n",
    "            out (np.array): (batch size, output_size)\n",
    "        \"\"\"\n",
    "        # Complete the forward pass through your entire MLP.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def zero_grads(self):\n",
    "        # Use numpyArray.fill(0.0) to zero out your backpropped derivatives in each\n",
    "        # of your linear and batchnorm layers.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def step(self):\n",
    "        # Apply a step to the weights and biases of the linear layers.\n",
    "        # Apply a step to the weights of the batchnorm layers.\n",
    "        # (You will add momentum later in the assignment to the linear layers only\n",
    "        # , not the batchnorm layers)\n",
    "\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            # Update weights and biases here\n",
    "            pass\n",
    "        # Do the same for batchnorm layers\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, labels):\n",
    "        # Backpropagate through the activation functions, batch norm and\n",
    "        # linear layers.\n",
    "        # Be aware of which return derivatives and which are pure backward passes\n",
    "        # i.e. take in a loss w.r.t it's output.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def error(self, labels):\n",
    "        return (np.argmax(self.output, axis = 1) != np.argmax(labels, axis = 1)).sum()\n",
    "\n",
    "    def total_loss(self, labels):\n",
    "        return self.criterion(self.output, labels).sum()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t (784, 64) 784 64\n",
      "t (64, 64) 64 64\n",
      "t (64, 32) 64 32\n",
      "t (32, 10) 32 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7fc1ed27e320>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight(in_feature,out_feature):\n",
    "    return np.random.randint((in_feature,out_feature))\n",
    "\n",
    "\n",
    "def bias(out_feature):\n",
    "    return np.random.randint(out_feature)\n",
    "\n",
    "MLP(784, 10, [64, 64, 32], [Sigmoid(), Sigmoid(), Sigmoid(), Identity()],\n",
    "weight, bias, Sigmoid(), 0.008, momentum=0.9, num_bn_layers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import math\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "\n",
    "    In all implementations, the state attribute must contain the result,\n",
    "    i.e. the output of forward (it will be tested).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an\n",
    "    # abstract base class for the others\n",
    "\n",
    "    # Note that these activation functions are scalar operations. I.e, they\n",
    "    # shouldn't change the shape of the input.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed\n",
    "    # to stay the same for AutoLab.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Might we need to store something before returning?\n",
    "        self.state = 1./(1+np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        # Maybe something we need later in here...\n",
    "        return self.state * (1-self.state)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1 - np.power(self.state,2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.where(x>0,x,0)\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return np.where(self.state>0,1.,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045176659730912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sigmoid()\n",
    "s.forward(x=3)\n",
    "s.derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-85b170da73e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "u = np.array(1.,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ([1,2,3],[5,6,7])\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_z(x,y):\n",
    "    return 20 + x**2+y**2-10*math.cos(2*math.pi*x)\n",
    "def der_z(x,y):\n",
    "    return (2*x + 20 * math.pi * math.sin(2*math.pi*x), 2 * y)\n",
    "\n",
    "lr = 0.001\n",
    "steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8 -0.2 20.189830056250532\n",
      "-1.743843356705169 -0.20040000000000002 23.467886650627648\n",
      "-1.6845461952796037 -0.20080080000000003 26.875647868839046\n",
      "-1.6303224491852346 -0.20120240160000002 29.5291217522063\n",
      "-1.5876935970887835 -0.20160480640320003 31.081455087392804\n",
      "-1.557974289046204 -0.20200801601600643 31.811956198092478\n",
      "-1.5387057028183517 -0.20241203204803845 32.11432120732432\n",
      "-1.5266528940576183 -0.20281685611213454 32.231908384605184\n",
      "-1.519233171559981 -0.2032224898243588 32.276439355644726\n",
      "-1.514697153433756 -0.20362893480400754 32.29316456265041\n",
      "-1.51193258708334 -0.20403619267361556 32.29947808403062\n",
      "-1.510250067844981 -0.2044442650589628 32.301921111448095\n",
      "-1.5092268001725495 -0.2048531535890807 32.302930308305704\n",
      "-1.5086046991557744 -0.20526285989625886 32.303409462129956\n",
      "-1.508226564159408 -0.2056733856160514 32.30369310681486\n",
      "-1.507996745925665 -0.2060847323872835 32.303904939547365\n",
      "-1.5078570788350898 -0.20649690185205807 32.30409067379792\n",
      "-1.5077722023901214 -0.20690989565576218 32.3042671991639\n",
      "-1.5077206236854366 -0.2073237154470737 32.30444075574226\n",
      "-1.5076892801653388 -0.20773836287796785 32.30461364987168\n",
      "-1.5076702333864644 -0.20815383960372377 32.30478673501836\n",
      "-1.5076586591286423 -0.20857014728293122 32.30496032809897\n",
      "-1.5076516257587718 -0.20898728757749707 32.30513454789475\n",
      "-1.5076473517740283 -0.20940526215265207 32.3053094400266\n",
      "-1.507644754594346 -0.20982407267695738 32.30548502310627\n",
      "-1.5076431763627514 -0.2102437208223113 32.30566130577894\n",
      "-1.5076422173170738 -0.21066420826395593 32.30583829301655\n",
      "-1.5076416345329084 -0.21108553668048385 32.30601598844177\n",
      "-1.5076412803919743 -0.2115077077538448 32.30619439518618\n",
      "-1.5076410651908807 -0.2119307231693525 32.30637351620721\n",
      "-1.507640934419474 -0.2123545846156912 32.306553354405175\n",
      "-1.50764085495352 -0.21277929378492258 32.30673391266656\n",
      "-1.507640806664388 -0.21320485237249243 32.30691519388\n",
      "-1.5076407773204978 -0.2136312620772374 32.30709720094223\n",
      "-1.5076407594890757 -0.21405852460139188 32.307279936760295\n",
      "-1.507640748653443 -0.21448664165059467 32.30746340425242\n",
      "-1.507640742068947 -0.21491561493389585 32.30764760634831\n",
      "-1.5076407380677417 -0.21534544616376364 32.307832545989385\n",
      "-1.5076407356363264 -0.21577613705609117 32.308018226128766\n",
      "-1.5076407341588267 -0.21620768933020335 32.30820464973142\n",
      "-1.5076407332609936 -0.21664010470886375 32.30839181977419\n",
      "-1.5076407327154067 -0.2170733849182815 32.3085797392458\n",
      "-1.5076407323838694 -0.21750753168811804 32.30876841114697\n",
      "-1.507640732182404 -0.21794254675149427 32.30895783849045\n",
      "-1.5076407320599794 -0.21837843184499725 32.309148024301\n",
      "-1.5076407319855856 -0.21881518870868724 32.30933897161554\n",
      "-1.5076407319403786 -0.2192528190861046 32.309530683483125\n",
      "-1.5076407319129077 -0.21969132472427683 32.30972316296503\n",
      "-1.5076407318962144 -0.2201307073737254 32.30991641313477\n",
      "-1.5076407318860703 -0.22057096878847285 32.31011043707821\n",
      "-1.507640731879906 -0.2210121107260498 32.3103052378935\n",
      "-1.5076407318761604 -0.22145413494750188 32.31050081869127\n",
      "-1.507640731873884 -0.2218970432173969 32.310697182594545\n",
      "-1.5076407318725007 -0.2223408373038317 32.31089433273889\n",
      "-1.5076407318716603 -0.22278551897843935 32.31109227227242\n",
      "-1.5076407318711496 -0.2232310900163962 32.31129100435582\n",
      "-1.5076407318708391 -0.223677552196429 32.31149053216251\n",
      "-1.5076407318706504 -0.22412490730082188 32.31169085887852\n",
      "-1.5076407318705358 -0.22457315711542353 32.31189198770271\n",
      "-1.507640731870466 -0.22502230342965437 32.31209392184671\n",
      "-1.507640731870424 -0.22547234803651367 32.31229666453502\n",
      "-1.5076407318703982 -0.2259232927325867 32.31250021900505\n",
      "-1.5076407318703826 -0.2263751393180519 32.31270458850719\n",
      "-1.507640731870373 -0.226827889596688 32.31290977630481\n",
      "-1.5076407318703673 -0.22728154537588138 32.31311578567437\n",
      "-1.5076407318703637 -0.22773610846663314 32.31332261990545\n",
      "-1.5076407318703617 -0.2281915806835664 32.31353028230079\n",
      "-1.5076407318703604 -0.22864796384493355 32.31373877617635\n",
      "-1.5076407318703597 -0.22910525977262342 32.3139481048614\n",
      "-1.5076407318703593 -0.22956347029216867 32.3141582716985\n",
      "-1.507640731870359 -0.230022597232753 32.31436928004362\n",
      "-1.5076407318703589 -0.23048264242721853 32.31458113326615\n",
      "-1.5076407318703586 -0.23094360771207295 32.31479383474899\n",
      "-1.5076407318703586 -0.2314054949274971 32.31500738788856\n",
      "-1.5076407318703586 -0.23186830591735208 32.3152217960949\n",
      "-1.5076407318703586 -0.2323320425291868 32.3154370627917\n",
      "-1.5076407318703586 -0.23279670661424517 32.31565319141636\n",
      "-1.5076407318703586 -0.23326230002747367 32.315870185420025\n",
      "-1.5076407318703586 -0.23372882462752861 32.31608804826769\n",
      "-1.5076407318703586 -0.23419628227678368 32.316306783438186\n",
      "-1.5076407318703586 -0.23466467484133724 32.31652639442431\n",
      "-1.5076407318703586 -0.2351340041910199 32.31674688473282\n",
      "-1.5076407318703586 -0.23560427219940194 32.31696825788453\n",
      "-1.5076407318703586 -0.23607548074380075 32.31719051741434\n",
      "-1.5076407318703586 -0.23654763170528834 32.3174136668713\n",
      "-1.5076407318703586 -0.2370207269686989 32.31763770981869\n",
      "-1.5076407318703586 -0.2374947684226363 32.31786264983404\n",
      "-1.5076407318703586 -0.23796975795948158 32.31808849050921\n",
      "-1.5076407318703586 -0.23844569747540054 32.31831523545045\n",
      "-1.5076407318703586 -0.23892258887035134 32.31854288827843\n",
      "-1.5076407318703586 -0.23940043404809205 32.31877145262833\n",
      "-1.5076407318703586 -0.23987923491618823 32.3190009321499\n",
      "-1.5076407318703586 -0.2403589933860206 32.31923133050746\n",
      "-1.5076407318703586 -0.24083971137279264 32.31946265138005\n",
      "-1.5076407318703586 -0.24132139079553822 32.319694898461414\n",
      "-1.5076407318703586 -0.2418040335771293 32.31992807546009\n",
      "-1.5076407318703586 -0.24228764164428354 32.320162186099466\n",
      "-1.5076407318703586 -0.24277221692757212 32.32039723411785\n",
      "-1.5076407318703586 -0.24325776136142727 32.320633223268494\n",
      "-1.5076407318703586 -0.24374427688415012 32.320870157319696\n",
      "res -1.5076407318703586 -0.2442317654379184 32.321108040054845\n"
     ]
    }
   ],
   "source": [
    "(x,y) = (-1.8,-0.2)\n",
    "for i in range(100):\n",
    "    print(x,y,func_z(x,y))\n",
    "    x = x + lr*der_z(x,y)[0]\n",
    "    y = y + lr*der_z(x,y)[1]\n",
    "print(\"res\",x,y,func_z(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4a46781fd4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(zip(([1,2,3],[4,5,6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-373c7e9a2671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 2"
     ]
    }
   ],
   "source": [
    "a = np.vstack([[1,2,3],[[4,7],[5,8],[6,9]]]).T\n",
    "np.random.shuffle(a)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9])\n",
    "a[1::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
