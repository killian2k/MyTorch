{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 2 3 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3])\n",
    "s = [5,a,7]\n",
    "\n",
    "print(np.hstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14747204 0.66815595 0.70853712]\n",
      " [0.65021236 0.20659561 0.18273222]]\n",
      "[0.79768439 0.87475155 0.89126934]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "in_feature = 2\n",
    "out_feature = 3\n",
    "\n",
    "x = np.random.random((batch_size,in_feature))\n",
    "W = np.random.random((in_feature, out_feature))\n",
    "b = np.random.random((1,out_feature))\n",
    "print(W)\n",
    "print(W.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-36c6c9f901dd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-36c6c9f901dd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    a.sum()?\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a.sum()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      "  18. 19.]\n",
      " [20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n",
      "  38. 39.]\n",
      " [40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57.\n",
      "  58. 59.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.5, 29.5, 49.5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "a.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-58a91c2b6b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "a = [1,2] + 4\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "a.reshape((1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "eps = 1e-8\n",
    "x = None\n",
    "norm = None\n",
    "out = None\n",
    "in_feature = 15\n",
    "\n",
    "# The following attributes will be tested\n",
    "var = np.ones((1, in_feature))\n",
    "mean = np.zeros((1, in_feature))\n",
    "\n",
    "gamma = np.ones((1, in_feature))\n",
    "dgamma = np.zeros((1, in_feature))\n",
    "\n",
    "beta = np.zeros((1, in_feature))\n",
    "dbeta = np.zeros((1, in_feature))\n",
    "\n",
    "# inference parameters\n",
    "running_mean = np.zeros((1, in_feature))\n",
    "running_var = np.ones((1, in_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.arange(150.0).reshape((10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (10, 15) [[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.]\n",
      " [ 15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n",
      "   29.]\n",
      " [ 30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.  43.\n",
      "   44.]\n",
      " [ 45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.\n",
      "   59.]\n",
      " [ 60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.\n",
      "   74.]\n",
      " [ 75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.\n",
      "   89.]\n",
      " [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103.\n",
      "  104.]\n",
      " [105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118.\n",
      "  119.]\n",
      " [120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      "  134.]\n",
      " [135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147. 148.\n",
      "  149.]]\n",
      "(1, 15)\n",
      "[67.5 68.5 69.5 70.5 71.5 72.5 73.5 74.5 75.5 76.5 77.5 78.5 79.5 80.5\n",
      " 81.5]\n",
      "var [1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25\n",
      " 1856.25 1856.25 1856.25 1856.25 1856.25 1856.25]\n",
      "[[-1.         -0.98540146 -0.97122302 -0.95744681 -0.94405594 -0.93103448\n",
      "  -0.91836735 -0.90604027 -0.89403974 -0.88235294 -0.87096774 -0.85987261\n",
      "  -0.8490566  -0.83850932 -0.82822086]\n",
      " [-0.77777778 -0.76642336 -0.75539568 -0.74468085 -0.73426573 -0.72413793\n",
      "  -0.71428571 -0.70469799 -0.69536424 -0.68627451 -0.67741935 -0.66878981\n",
      "  -0.66037736 -0.65217391 -0.64417178]\n",
      " [-0.55555556 -0.54744526 -0.53956835 -0.53191489 -0.52447552 -0.51724138\n",
      "  -0.51020408 -0.5033557  -0.49668874 -0.49019608 -0.48387097 -0.47770701\n",
      "  -0.47169811 -0.46583851 -0.4601227 ]\n",
      " [-0.33333333 -0.32846715 -0.32374101 -0.31914894 -0.31468531 -0.31034483\n",
      "  -0.30612245 -0.30201342 -0.29801325 -0.29411765 -0.29032258 -0.2866242\n",
      "  -0.28301887 -0.27950311 -0.27607362]\n",
      " [-0.11111111 -0.10948905 -0.10791367 -0.10638298 -0.1048951  -0.10344828\n",
      "  -0.10204082 -0.10067114 -0.09933775 -0.09803922 -0.09677419 -0.0955414\n",
      "  -0.09433962 -0.0931677  -0.09202454]\n",
      " [ 0.11111111  0.10948905  0.10791367  0.10638298  0.1048951   0.10344828\n",
      "   0.10204082  0.10067114  0.09933775  0.09803922  0.09677419  0.0955414\n",
      "   0.09433962  0.0931677   0.09202454]\n",
      " [ 0.33333333  0.32846715  0.32374101  0.31914894  0.31468531  0.31034483\n",
      "   0.30612245  0.30201342  0.29801325  0.29411765  0.29032258  0.2866242\n",
      "   0.28301887  0.27950311  0.27607362]\n",
      " [ 0.55555556  0.54744526  0.53956835  0.53191489  0.52447552  0.51724138\n",
      "   0.51020408  0.5033557   0.49668874  0.49019608  0.48387097  0.47770701\n",
      "   0.47169811  0.46583851  0.4601227 ]\n",
      " [ 0.77777778  0.76642336  0.75539568  0.74468085  0.73426573  0.72413793\n",
      "   0.71428571  0.70469799  0.69536424  0.68627451  0.67741935  0.66878981\n",
      "   0.66037736  0.65217391  0.64417178]\n",
      " [ 1.          0.98540146  0.97122302  0.95744681  0.94405594  0.93103448\n",
      "   0.91836735  0.90604027  0.89403974  0.88235294  0.87096774  0.85987261\n",
      "   0.8490566   0.83850932  0.82822086]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape\", x.shape,x)\n",
    "mean = x.mean(axis=0)\n",
    "print(running_mean.shape)\n",
    "print(mean)\n",
    "var = np.var(x,axis=0)\n",
    "print(\"var\",var)\n",
    "#print(np.subtract(x,mean).shape)\n",
    "norm = ((x - mean)/(np.sqrt(np.power(mean,2) + eps)))\n",
    "out = gamma * norm + beta\n",
    "# Update running batch statistics\n",
    "running_mean = alpha * running_mean+ (1-alpha) * mean\n",
    "running_var = alpha * var + (1-alpha) * var\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b07936417e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mytorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbatchnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append('mytorch')\n",
    "from loss import *\n",
    "from activation import *\n",
    "from batchnorm import *\n",
    "from linear import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_feature, out_feature, weight_init_fn, bias_init_fn):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            W (np.array): (in feature, out feature)\n",
    "            dW (np.array): (in feature, out feature)\n",
    "            momentum_W (np.array): (in feature, out feature)\n",
    "\n",
    "            b (np.array): (1, out feature)\n",
    "            db (np.array): (1, out feature)\n",
    "            momentum_B (np.array): (1, out feature)\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = weight_init_fn(in_feature, out_feature)\n",
    "        self.b = bias_init_fn(out_feature)\n",
    "\n",
    "        # TODO: Complete these but do not change the names.\n",
    "        self.dW = np.zeros(None)\n",
    "        self.db = np.zeros(None)\n",
    "        \n",
    "        self.in_feature = 0\n",
    "        self.x = np.zeros(None)\n",
    "\n",
    "        self.momentum_W = np.zeros(None)\n",
    "        self.momentum_b = np.zeros(None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, in feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, out feature)\n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        self.in_feature = x.shape[0]\n",
    "        self.x = x\n",
    "        return x.dot(self.W) + self.b\n",
    "        \n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            delta (np.array): (batch size, out feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, in feature)\n",
    "        \"\"\"\n",
    "        #raise NotImplemented\n",
    "        self.dW = 1/ self.in_feature * self.x.T.dot(delta)\n",
    "        self.db = (1/self.in_feature * delta.sum(axis=0, keepdims=True))\n",
    "        out = delta.dot(self.W.T)\n",
    "        return out\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import math\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "\n",
    "    In all implementations, the state attribute must contain the result,\n",
    "    i.e. the output of forward (it will be tested).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an\n",
    "    # abstract base class for the others\n",
    "\n",
    "    # Note that these activation functions are scalar operations. I.e, they\n",
    "    # shouldn't change the shape of the input.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed\n",
    "    # to stay the same for AutoLab.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Might we need to store something before returning?\n",
    "        self.state = 1./(1+np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        # Maybe something we need later in here...\n",
    "        return self.state * (1-self.state)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1 - np.power(self.state,2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.where(x>0,x,0)\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return np.where(self.state>0,1.,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('mytorch')\n",
    "\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn,\n",
    "                 bias_init_fn, criterion, lr, momentum=0.0, num_bn_layers=0):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.num_bn_layers = num_bn_layers\n",
    "        self.bn = num_bn_layers > 0\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        # Initialize and add all your linear layers into the list 'self.linear_layers'\n",
    "        # (HINT: self.foo = [ bar(???) for ?? in ? ])\n",
    "        # (HINT: Can you use zip here?)\n",
    "        #Linear: (in_feature, out_feature, weight_init_fn, bias_init_fn)\n",
    "        activations\n",
    "        self.linear_layers = [Linear((t[0],t[1],weight_init_fn,bias_init_fn)) for t in zip(np.hstack((self.input_size,hiddens)), np.hstack((hiddens,self.output_size)))]\n",
    "\n",
    "        # If batch norm, add batch norm layers into the list 'self.bn_layers'\n",
    "        if self.bn:\n",
    "            BatchNorm(_in_feature)\n",
    "            self.bn_layers = self.num_bn_layers#NOT SURE\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, input_size)\n",
    "        Return:\n",
    "            out (np.array): (batch size, output_size)\n",
    "        \"\"\"\n",
    "        # Complete the forward pass through your entire MLP.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def zero_grads(self):\n",
    "        # Use numpyArray.fill(0.0) to zero out your backpropped derivatives in each\n",
    "        # of your linear and batchnorm layers.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def step(self):\n",
    "        # Apply a step to the weights and biases of the linear layers.\n",
    "        # Apply a step to the weights of the batchnorm layers.\n",
    "        # (You will add momentum later in the assignment to the linear layers only\n",
    "        # , not the batchnorm layers)\n",
    "\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            # Update weights and biases here\n",
    "            pass\n",
    "        # Do the same for batchnorm layers\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self, labels):\n",
    "        # Backpropagate through the activation functions, batch norm and\n",
    "        # linear layers.\n",
    "        # Be aware of which return derivatives and which are pure backward passes\n",
    "        # i.e. take in a loss w.r.t it's output.\n",
    "        raise NotImplemented\n",
    "\n",
    "    def error(self, labels):\n",
    "        return (np.argmax(self.output, axis = 1) != np.argmax(labels, axis = 1)).sum()\n",
    "\n",
    "    def total_loss(self, labels):\n",
    "        return self.criterion(self.output, labels).sum()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t (784, 64) 784 64\n",
      "t (64, 64) 64 64\n",
      "t (64, 32) 64 32\n",
      "t (32, 10) 32 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7fc1ed27e320>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight(in_feature,out_feature):\n",
    "    return np.random.randint((in_feature,out_feature))\n",
    "\n",
    "\n",
    "def bias(out_feature):\n",
    "    return np.random.randint(out_feature)\n",
    "\n",
    "MLP(784, 10, [64, 64, 32], [Sigmoid(), Sigmoid(), Sigmoid(), Identity()],\n",
    "weight, bias, Sigmoid(), 0.008, momentum=0.9, num_bn_layers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries as they will not\n",
    "# be available to AutoLab and are not needed (or allowed)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import math\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "\n",
    "    In all implementations, the state attribute must contain the result,\n",
    "    i.e. the output of forward (it will be tested).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an\n",
    "    # abstract base class for the others\n",
    "\n",
    "    # Note that these activation functions are scalar operations. I.e, they\n",
    "    # shouldn't change the shape of the input.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed\n",
    "    # to stay the same for AutoLab.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Might we need to store something before returning?\n",
    "        self.state = 1./(1+np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        # Maybe something we need later in here...\n",
    "        return self.state * (1-self.state)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1 - np.power(self.state,2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.where(x>0,x,0)\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return np.where(self.state>0,1.,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045176659730912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sigmoid()\n",
    "s.forward(x=3)\n",
    "s.derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-85b170da73e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "u = np.array(1.,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack(([1,2,3],[5,6,7]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
